{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "########################################\n",
    "## import packages\n",
    "########################################\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Flatten, Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model,load_model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D,GlobalAveragePooling1D ,Conv1D, MaxPooling1D, GRU, CuDNNGRU\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim=70,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        #self.init = initializations.get('glorot_uniform')\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # eij = K.dot(x, self.W) TF backend doesn't support it\n",
    "\n",
    "        # features_dim = self.W.shape[0]\n",
    "        # step_dim = x._keras_shape[1]\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        #print weigthted_input.shape\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        #return input_shape[0], input_shape[-1]\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model.hdf5', 'tokenizer.pickle']\n"
     ]
    }
   ],
   "source": [
    "# All submission files were downloaded from different public kernels\n",
    "# See the description to see the source of each submission file\n",
    "model_path = \"../model/\"\n",
    "sub_path = \"D:/OneDrive/Documentos/Toxic/sub/\"\n",
    "all_files = os.listdir(model_path)\n",
    "print (all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../model/\n"
     ]
    }
   ],
   "source": [
    "print (model_path)\n",
    "#load_model(model_path+\"model.hdf5\")\n",
    "#model = load_model(model_path+\"model.hdf5\", custom_objects={'Attention':Attention()})\n",
    "model=load_model(model_path+\"model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 70)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 70, 300)           28500300  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 70, 100)           160400    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 28,665,801\n",
      "Trainable params: 165,501\n",
      "Non-trainable params: 28,500,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and concatenate submissions\n",
    "models = [ load_model(model_path+f) \\\n",
    "          for f in all_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_several_folds_mean(data, nfolds):\n",
    "    a = np.array(data[0])\n",
    "    for i in range(1, nfolds):\n",
    "        a += np.array(data[i])\n",
    "    a /= nfolds\n",
    "    return a.tolist()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "batch_size = 64\n",
    "num_fold = 0\n",
    "yfull_test = []\n",
    "test_id = []\n",
    "nfolds = len(models)\n",
    "print (\"***********************************************************\")\n",
    "\n",
    "for i in range(nfolds):\n",
    "    print (\"\\n\\nGererating Predictions - Model:\",all_files[i])\n",
    "    model = models[i]\n",
    "    num_fold += 1\n",
    "    print('Start KFold number {} from {}'.format(num_fold, nfolds))\n",
    "    test_prediction = model.predict(test_data, batch_size=batch_size, verbose=1)\n",
    "    yfull_test.append(test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res = merge_several_folds_mean(yfull_test, nfolds)\n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "info_string=\"BestModelsEssemble\"+\"10-03-Test\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "sample_submission = pd.read_csv(path+\"data/sample_submission.csv\")\n",
    "sample_submission[list_classes] = test_res\n",
    "\n",
    "sample_submission.to_csv(path+'sub/CV-'+info_string+'.csv', index=False)\n",
    "sample_submission.to_csv(path+'sub/CV-'+info_string+'.gz', index=False, compression = 'gzip')\n",
    "print (\"Arquivo Salvo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "## process texts in datasets\n",
    "########################################\n",
    "import re\n",
    "def remove_urls (vTEXT):\n",
    "    vTEXT = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', vTEXT, flags=re.MULTILINE)\n",
    "    return(vTEXT)\n",
    "\n",
    "def ReplaceThreeOrMore(s):\n",
    "    # pattern to look for three or more repetitions of any character, including\n",
    "    # newlines.\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL) \n",
    "    return pattern.sub(r\"\\1\", s)\n",
    "\n",
    "def splitstring(s):\n",
    "    # searching the number of characters to split on\n",
    "    proposed_pattern = s[0]\n",
    "    for i, c in enumerate(s[1:], 1):\n",
    "        if c != \" \":\n",
    "            if proposed_pattern == s[i:(i+len(proposed_pattern))]:\n",
    "                # found it\n",
    "                break\n",
    "            else:\n",
    "                proposed_pattern += c\n",
    "    else:\n",
    "        exit(1)\n",
    "\n",
    "    return proposed_pattern\n",
    "\n",
    "def clean_numbers(x):\n",
    "\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "#Regex to remove all Non-Alpha Numeric and space\n",
    "special_character_removal=re.compile(r'[^a-z\\d ]',re.IGNORECASE)\n",
    "\n",
    "#regex to replace all numerics\n",
    "replace_numbers=re.compile(r'\\d+',re.IGNORECASE)\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "\n",
    "mispell_dict = {'colour':'color',\n",
    "                'centre':'center',\n",
    "                'didnt':'did not',\n",
    "                'doesnt':'does not',\n",
    "                'isnt':'is not',\n",
    "                'shouldnt':'should not',\n",
    "                'favourite':'favorite',\n",
    "                'travelling':'traveling',\n",
    "                'counselling':'counseling',\n",
    "                'theatre':'theater',\n",
    "                'cancelled':'canceled',\n",
    "                'labour':'labor',\n",
    "                'organisation':'organization',\n",
    "                'wwii':'world war 2',\n",
    "                'citicise':'criticize',\n",
    "                'instagram': 'social medium',\n",
    "                'whatsapp': 'social medium',\n",
    "                'snapchat': 'social medium'\n",
    "\n",
    "                }\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "\n",
    "    return mispellings_re.sub(replace, text)\n",
    "\n",
    "def clean_text(x):\n",
    "\n",
    "    x = str(x)\n",
    "    for punct in \"/-'\":\n",
    "        x = x.replace(punct, ' ')\n",
    "    for punct in '&':\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
    "        x = x.replace(punct, '')\n",
    "    return x\n",
    "\n",
    "def text_to_wordlist(text,to_lower=False, rem_urls=False, rem_3plus=False,\n",
    "                     clean_t=True, clean_num=True,mispelling=True,rem_specwords= False,\n",
    "                     split_repeated=True, rem_special=False, rep_num=False, \n",
    "                     man_adj=True, rem_stopwords=False, stem_snowball=False,\n",
    "                     stem_porter=False, lemmatize=False):\n",
    "\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    if rem_urls:\n",
    "        text = remove_urls(text)\n",
    "    if to_lower:    \n",
    "        text = text.lower()\n",
    "    if rem_3plus:    \n",
    "        text = ReplaceThreeOrMore(text)\n",
    "        \n",
    "    if clean_t:\n",
    "        text= clean_text(text)\n",
    "        \n",
    "    if clean_num:\n",
    "        text= clean_numbers(text)    \n",
    "        \n",
    "    if mispelling:\n",
    "        text= replace_typical_misspell(text)\n",
    "\n",
    "    if man_adj: \n",
    "        # Clean the text\n",
    "        text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "        text = re.sub(r\"what's\", \"what is \", text)\n",
    "        text = re.sub(r\"\\'s\", \" \", text)\n",
    "        text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "        text = re.sub(r\"can't\", \"cannot \", text)\n",
    "        text = re.sub(r\"n't\", \" not \", text)\n",
    "        text = re.sub(r\"i'm\", \"i am \", text)\n",
    "        text = re.sub(r\"\\'re\", \" are \", text)\n",
    "        text = re.sub(r\"\\'d\", \" would \", text)\n",
    "        text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "        text = re.sub(r\",\", \" \", text)\n",
    "        text = re.sub(r\"\\.\", \" \", text)\n",
    "        text = re.sub(r\"!\", \" ! \", text)\n",
    "        text = re.sub(r\"\\/\", \" \", text)\n",
    "        text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "        text = re.sub(r\"\\+\", \" + \", text)\n",
    "        text = re.sub(r\"\\-\", \" - \", text)\n",
    "        text = re.sub(r\"\\=\", \" = \", text)\n",
    "        text = re.sub(r\"'\", \" \", text)\n",
    "        text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "        text = re.sub(r\":\", \" : \", text)\n",
    "        text = re.sub(r\" e g \", \" eg \", text)\n",
    "        text = re.sub(r\" b g \", \" bg \", text)\n",
    "        text = re.sub(r\" u s \", \" american \", text)\n",
    "        text = re.sub(r\"\\0s\", \"0\", text)\n",
    "        text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "        text = re.sub(r\"e - mail\", \"email\", text)\n",
    "        text = re.sub(r\"j k\", \"jk\", text)\n",
    "        text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    # split them into a list\n",
    "    text = text.split()\n",
    "    \n",
    "    if split_repeated:\n",
    "        for i, c in enumerate(text):\n",
    "            text[i]=splitstring(c)\n",
    "    \n",
    "    if rem_specwords:    \n",
    "        to_remove = ['a','to','of','and']\n",
    "        text = [w for w in text if not w in to_remove]\n",
    "        \n",
    "    # Optionally, remove stop words\n",
    "    if rem_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    #Remove Special Characters\n",
    "    if rem_special: \n",
    "        text=special_character_removal.sub('',text)\n",
    "    \n",
    "    #Replace Numbers\n",
    "    if rep_num:     \n",
    "        text=replace_numbers.sub('n',text)\n",
    "\n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_snowball:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    if stem_porter:\n",
    "        st = PorterStemmer()\n",
    "        txt = \" \".join([st.stem(w) for w in text.split()])\n",
    "        \n",
    "    if lemmatize:\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        txt = \" \".join([wordnet_lemmatizer.lemmatize(w) for w in text.split()])   \n",
    " \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_tokenizer( file):\n",
    "    # saving\n",
    "    with open(file, 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_tokenizer( file ):\n",
    "    # loading\n",
    "    with open(file, 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../model/tokenizer.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "maxlen=70\n",
    "max_features=95000\n",
    "## Tokenize the sentences\n",
    "#tokenizer = Tokenizer(num_words=max_features)\n",
    "print (model_path+'tokenizer.pickle')\n",
    "tokenizer=load_tokenizer(model_path+'tokenizer.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_question(question, PreProcess=False):\n",
    "    global tokenizer\n",
    "    print(question)\n",
    "\n",
    "    questions=[question]\n",
    "    \n",
    "    print (questions)\n",
    "    if PreProcess:\n",
    "        prep_questions = [text_to_wordlist(questions)]\n",
    "    else:\n",
    "        prep_questions = questions\n",
    "    print (\"_________________\")\n",
    "    print (prep_questions)    \n",
    "    question_ = tokenizer.texts_to_sequences(prep_questions)\n",
    "    print (question_)\n",
    "    ## Pad the sentences \n",
    "    questions_pad = pad_sequences(question_, maxlen=maxlen)\n",
    "    print (questions_pad)\n",
    "    #print(len(tokenizer.word_index))\n",
    "    #print(len(tokenizer.word_counts))\n",
    "    \n",
    "    return questions_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHAT IS THE biggest country in the world?\n",
      "['WHAT IS THE biggest country in the world?']\n",
      "_________________\n",
      "['WHAT IS THE biggest country in the world?']\n",
      "[[2, 3, 1, 643, 130, 6, 1, 95]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   2   3   1 643 130   6   1  95]]\n"
     ]
    }
   ],
   "source": [
    "q=prepare_question(\"WHAT IS THE biggest country in the world?\",False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the biggest country in the world?\n",
      "['What is the biggest country in the world?']\n",
      "_________________\n",
      "['What is the biggest country in the world']\n",
      "[[2, 3, 1, 643, 130, 6, 1, 95]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   2   3   1 643 130   6   1  95]]\n"
     ]
    }
   ],
   "source": [
    "q1=prepare_question(\"What is the biggest country in the world?\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "test_prediction = model.predict(q, batch_size=512, verbose=1)\n",
    "test_prediction1 = model.predict(q1, batch_size=512, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00126187]] [[0.00126187]]\n"
     ]
    }
   ],
   "source": [
    "print (test_prediction,test_prediction1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1306122, 3)\n",
      "Test shape :  (56370, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../input/train.csv\")\n",
    "test_df = pd.read_csv(\"../input/test.csv\")\n",
    "print(\"Train shape : \",train_df.shape)\n",
    "print(\"Test shape : \",test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split to train and val\n",
    "#train_fit, train_val = train_test_split(train_df, test_size=0.08, random_state=2018)\n",
    "train_X=train_df[\"question_text\"].values\n",
    "test_X=test_df[\"question_text\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do you have an adopted dog, how would you encourage people to adopt and not shop?'\n",
      " 'Why does velocity affect time? Does velocity affect space geometry?'\n",
      " 'How did Otto von Guericke used the Magdeburg hemispheres?'\n",
      " 'Can I convert montra helicon D to a mountain bike by just changing the tyres?'\n",
      " 'Is Gaza slowly becoming Auschwitz, Dachau or Treblinka for Palestinians?'\n",
      " 'Why does Quora automatically ban conservative opinions when reported, but does not do the same for liberal views?'\n",
      " 'Is it crazy if I wash or wipe my groceries off? Germs are everywhere.'\n",
      " 'Is there such a thing as dressing moderately, and if so, how is that different than dressing modestly?'\n",
      " 'Is it just me or have you ever been in this phase wherein you became ignorant to the people you once loved, completely disregarding their feelings/lives so you get to have something go your way and feel temporarily at ease. How did things change?']\n"
     ]
    }
   ],
   "source": [
    "print (train_X[1:10])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_questions = []\n",
    "for text in train_X:\n",
    "    train_questions.append(text_to_wordlist(text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do you have an adopted dog how would you encourage people to adopt and not shop'\n",
      " 'Why does velocity affect time Does velocity affect space geometry'\n",
      " 'How did Otto von Guericke used the Magdeburg hemispheres'\n",
      " 'Can I convert montra helicon D to a mountain bike by just changing the tyres'\n",
      " 'Is Gaza slowly becoming Auschwitz Dachau or Treblinka for Palestinians'\n",
      " 'Why does Quora automatically ban conservative opinions when reported but does not do the same for liberal views'\n",
      " 'Is it crazy if I wash or wipe my groceries off Germs are everywhere'\n",
      " 'Is there such a thing as dressing moderately and if so how is that different than dressing modestly'\n",
      " 'Is it just me or have you ever been in this phase wherein you became ignorant to the people you once loved completely disregarding their feelings lives so you get to have something go your way and feel temporarily at ease How did things change']\n"
     ]
    }
   ],
   "source": [
    "print (np.array(train_questions[1:10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = tokenizer.texts_to_sequences(train_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([11, 14, 24, 30, 3716, 466, 9, 36, 14, 3624, 38, 5, 3043, 10, 43, 1806])\n",
      " list([16, 26, 1979, 371, 72, 26, 1979, 371, 447, 5416])\n",
      " list([9, 50, 12458, 8087, 50016, 121, 1, 38804, 26838])\n",
      " list([15, 7, 1112, 41663, 91364, 563, 5, 4, 3015, 1499, 49, 97, 1452, 1, 9106])\n",
      " list([3, 8995, 3868, 763, 18687, 45334, 23, 91365, 13, 2572])\n",
      " list([16, 26, 107, 2493, 1623, 1443, 2372, 33, 4219, 66, 26, 43, 11, 1, 140, 13, 1130, 891])\n",
      " list([3, 17, 1927, 20, 7, 2893, 23, 5773, 18, 12228, 222, 12015, 12, 3613])\n",
      " list([3, 40, 204, 4, 184, 37, 6372, 19024, 10, 20, 54, 9, 3, 19, 129, 73, 6372, 24411])\n",
      " list([3, 17, 97, 56, 23, 24, 14, 92, 117, 6, 65, 2134, 14977, 14, 1339, 3101, 5, 1, 38, 14, 652, 1691, 895, 18338, 57, 1085, 1032, 54, 14, 34, 5, 24, 192, 109, 29, 81, 10, 103, 6876, 46, 6462, 9, 50, 146, 166])]\n"
     ]
    }
   ],
   "source": [
    "print (np.array(train_tokens[1:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    \n",
    "    ## Tokenize the sentences\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    if PreProcess:\n",
    "   \n",
    "\n",
    "    else:\n",
    "        tokenizer.fit_on_texts(list(train_X)+list(test_X))\n",
    "        train_X = tokenizer.texts_to_sequences(train_X)\n",
    "        test_X = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "    print(len(train_X), 'train sequences')\n",
    "    print(len(test_X), 'test sequences')\n",
    "    print('Average train sequence length: {}'.format(np.mean(list(map(len, train_X)), dtype=int)))\n",
    "    print('Average test sequence length: {}'.format(np.mean(list(map(len, test_X)), dtype=int)))\n",
    "    print('Max train sequence length: {}'.format(np.max(list(map(len, train_X)))))\n",
    "    print('Max test sequence length: {}'.format(np.max(list(map(len, test_X)))))  \n",
    "    \n",
    "    ## Pad the sentences \n",
    "    train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "    test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "\n",
    "    ## Get the target values\n",
    "    train_y = train_df['target'].values\n",
    "  \n",
    "    print(len(tokenizer.word_index))\n",
    "    print(len(tokenizer.word_counts))\n",
    "    \n",
    "    return train_X, test_X, train_y, test_df, tokenizer.word_index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vsenti",
   "language": "python",
   "name": "vsenti"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
